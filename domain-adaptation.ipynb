{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import scipy.linalg as sc_linalg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scenarios and Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = pd.read_csv('MALE.csv', sep=',')\n",
    "df_female = pd.read_csv('FEMALE.csv', sep=',')\n",
    "df_mixed = pd.read_csv('MIXED.csv', sep=',')\n",
    "\n",
    "# Removing VR Band of student 0\n",
    "df_mixed = df_mixed[df_mixed['VR Band of Student'] != 0]\n",
    "\n",
    "def one_hot_encoding_transformation(df):\n",
    "    df_year = pd.get_dummies(df['Year'], prefix='Year')\n",
    "    df_vr_band_student = pd.get_dummies(df['VR Band of Student'], prefix='VR Band of Student')\n",
    "    df_ethnic_group = pd.get_dummies(df['Ethnic group of student'], prefix='Ethnic group of student')\n",
    "    df_school_denomination = pd.get_dummies(df['School denomination'], prefix='School denomination')\n",
    "    df_final = pd.concat([df_year, df['FSM'], df['VR1 Band'],df_vr_band_student,df_ethnic_group, df_school_denomination], axis = 1)\n",
    "    return df_final, df['Exam Score']\n",
    "\n",
    "df_male, y_male = one_hot_encoding_transformation(df_male)\n",
    "df_female, y_female = one_hot_encoding_transformation(df_female)\n",
    "df_mixed, y_mixed = one_hot_encoding_transformation(df_mixed)\n",
    "\n",
    "# Splitting each dataset (domain) into train - 60%, test - 20% and dev - 20%\n",
    "def split_dataset(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    return X_train, y_train, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "def create_domains(domain):\n",
    "    if domain == 'male':\n",
    "        X_male_train, y_male_train, X_male_test, y_male_test, X_male_dev, y_male_dev = split_dataset(df_male, y_male)\n",
    "        return X_male_train, y_male_train, X_male_test, y_male_test, X_male_dev, y_male_dev\n",
    "    if domain == 'female':\n",
    "        X_female_train, y_female_train, X_female_test, y_female_test, X_female_dev, y_female_dev= split_dataset(df_female, y_female)\n",
    "        return X_female_train, y_female_train, X_female_test, y_female_test, X_female_dev, y_female_dev\n",
    "    if domain == 'mixed':\n",
    "        X_mixed_train, y_mixed_train, X_mixed_test, y_mixed_test, X_mixed_dev, y_mixed_dev= split_dataset(df_mixed, y_mixed)\n",
    "        return X_mixed_train, y_mixed_train, X_mixed_test, y_mixed_test, X_mixed_dev, y_mixed_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_linear_model(X_train, y_train, X_test, y_test, X_dev, y_dev):\n",
    "    print('--> Linear Model Lasso')\n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    X_dev = preprocessing.scale(X_dev)\n",
    "    lasso = linear_model.Lasso()\n",
    "    parameters = { 'alpha': np.logspace(-4, -0.5, 30)}\n",
    "    cv = KFold(n_splits = 10, random_state = 42)\n",
    "    lasso_regressor = GridSearchCV(lasso, parameters, cv = cv)\n",
    "    lasso_regressor.fit(X_dev, y_dev)    \n",
    "    y_dev_pred = lasso_regressor.predict(X_dev)\n",
    "    print('MSE Dev:', mean_squared_error(y_dev, y_dev_pred))\n",
    "    print(\"Best: %f using %s\" % (lasso_regressor.best_score_, lasso_regressor.best_params_))\n",
    "    lasso = linear_model.Lasso()\n",
    "    lasso.set_params = lasso_regressor.best_params_\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_train_pred = lasso.predict(X_train)\n",
    "    print('MSE Train:', mean_squared_error(y_train, y_train_pred))\n",
    "    y_test_pred = lasso.predict(X_test)\n",
    "    return y_test_pred\n",
    "    \n",
    "def evaluate_on_neural_network(X_train, y_train, X_test, y_test, X_dev, y_dev):\n",
    "    print('--> Multi-layer Perceptron regressor')\n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    X_dev = preprocessing.scale(X_dev)\n",
    "  \n",
    "    param_grid = {'hidden_layer_sizes': [100, 200],\n",
    "          'solver': ['adam'],\n",
    "          'learning_rate': ['invscaling'],\n",
    "          'learning_rate_init': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "          'alpha': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "          'epsilon': [1, 1e-1, 1e-2, 1e-3, 1e-4],        \n",
    "          'early_stopping': [True]}\n",
    "    \n",
    "    regr = MLPRegressor(random_state=1, max_iter=500)\n",
    "    cv = KFold(n_splits = 10, random_state = 42)\n",
    "    model_grid = GridSearchCV(estimator=regr, param_grid=param_grid, n_jobs=-1,scoring='neg_mean_squared_error', cv= cv)\n",
    "    grid_result = model_grid.fit(X_dev, y_dev)\n",
    "    y_dev_pred = grid_result.predict(X_dev)\n",
    "    print('MSE Dev:', mean_squared_error(y_dev, y_dev_pred))\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    final_model = MLPRegressor(random_state=1, max_iter=500)\n",
    "    final_model.set_params =  grid_result.best_params_\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_train_pred = final_model.predict(X_train)\n",
    "    print('MSE Train:', mean_squared_error(y_train, y_train_pred))\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    return y_test_pred\n",
    "\n",
    "# Method to concatenate X and y\n",
    "def add_prediction_features(X, y):\n",
    "    df_y = pd.DataFrame(y)\n",
    "    df_y.reset_index(drop=True, inplace=True)  \n",
    "    X.reset_index(drop=True, inplace=True)  \n",
    "    return pd.concat([X, df_y], axis = 1)\n",
    "\n",
    "def get_mse(y_test_pred_lin, y_test_pred_nn, y_test):\n",
    "    print()\n",
    "    print('--> Linear Model Lasso')\n",
    "    print('MSE Test:', mean_squared_error(y_test, y_test_pred_lin))\n",
    "    print('--> Multi-layer Perceptron regressor')\n",
    "    print('MSE Test:', mean_squared_error(y_test, y_test_pred_nn))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline methods and FEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 100\n",
      "****************************************************************************************************\n",
      "Scenario 1: Male is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73136473963345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.8480412515774\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51379907076607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.93399728006057\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.16693598803016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.41524325238066\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.1367248438925\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 74.20861306916636\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.04945767054204\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.59020507474008\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.13645448884407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 59.1681132527683\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.04993467064884\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 159.73315764755844\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.13645448884407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 59.1681132527683\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.47223255500407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.09990884014104\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.13645448884407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 59.1681132527683\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.04993467064884\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 159.73315764755844\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.13645448884407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 59.1681132527683\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.47223255500407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.12954135166734\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.13645448884407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 59.1681132527683\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.47223255500407\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.11303225069227\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.24303051142361\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 94.00750589215446\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74893229158114\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.0306903003472\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 111.26226328970931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.39805361143429\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.36396902786089\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.8834165188848\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 111.26226328970931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 87.39805361143429\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.36396902786089\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 248.5478678596884\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 111.26226328970931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.39805361143429\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.36396902786089\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.8834165188848\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 111.26226328970931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.39805361143429\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.50193755708167\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.84375524881546\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 111.26226328970931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.39805361143429\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.34191912430663\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 173.62401838942395\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 128.74891535684188\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 74.77379212262264\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.33305031813026\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 134.80191829174453\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.6900395104767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 58.743531955541016\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.47739871118627\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 258.55829164685616\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.6900395104767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 58.743531955541016\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.42254324756384\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 295.6207861442075\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.6900395104767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 58.743531955541016\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.47739871118627\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 258.55829164685616\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.6900395104767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 58.743531955541016\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.27612079722742\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.59842449462556\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.6900395104767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 58.743531955541016\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.42254324756384\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 250.99268917040948\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 145.63173172447603\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.8990896499388\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.46317856489064\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.84477803542117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.2396417012065\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.17262553197014\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.9917519203369\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.2693354001331\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.68829476120067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.83573699045365\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06555563300707\n",
      "Best: 0.201138 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.41721791638774\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 104.13771339479615\n",
      "Best: -107.459044 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.03691891076595\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.41981359344928\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.5948241481598\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.04945767054204\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.59020507474008\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.798141397186\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 142.3638168531547\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.9067155638889\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 136.23791126251805\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.37518017065071\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.21248830283005\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.20353521747144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.28754797409076\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.39178070435112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.46309027630015\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.93991663128972\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.7391152094582\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.84794299828727\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.11562277356498\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.11585980534373\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.59261296862043\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.7436670524591\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.17008579462457\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.04993467064884\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 159.73315764755844\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.94861632009895\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 155.03883360503522\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.18567267859906\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.19992071820195\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.76110374614923\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 154.2164189870586\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.6749095227494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 158.08832841160518\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.92709000839959\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 164.8156489918416\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.51764520309983\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 174.398380727768\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.44657510685008\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 186.83652361938428\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.71387971965034\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 202.13007766669045\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Linear Model Lasso\n",
      "MSE Test: 122.31955904150063\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 220.27904286968658\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 2: Female is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874624\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 152.50434075622093\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.0711292058723\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.3153082208075\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44152891710796\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 53.39636007542126\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65038633086903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.8757444623967\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44134043603837\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 44.418364466799055\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65025555929589\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 157.9432388428203\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44134043603837\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 44.418364466799055\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.58339681338725\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 283.9116353652217\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44134043603837\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 44.418364466799055\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65025555929589\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 157.9432388428203\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44134043603837\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 44.418364466799055\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.58339681338725\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 283.9156017124817\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44134043603837\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 44.418364466799055\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.58339681338725\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 283.8737247045144\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601538\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.88907537066027\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.18562987361278\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.8564304981384\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601535\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.20114302881059\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.1856298736129\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 337.82039544572433\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601535\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 91.20114302881059\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.1856298736129\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 362.2379014634781\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601535\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.20114302881059\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.1856298736129\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 337.82039544572433\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601535\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.20114302881059\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.1856298736129\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 329.11418202961636\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.81807973601535\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.20114302881059\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.1856298736129\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 251.3671511539592\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 106.19734884528258\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 63.914733287249376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.88761973097252\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 154.22813392552527\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.54130521773955\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 52.15779318480684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.06048292028498\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 326.5627059118652\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.54130521773955\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 52.15779318480684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 138.2354064554608\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 414.6809045593847\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.54130521773955\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 52.15779318480684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.06048292028498\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 326.5627059118652\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.54130521773955\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 52.15779318480684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 138.2354064554608\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 314.49486559488736\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.54130521773955\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 52.15779318480684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 138.2354064554608\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 304.63577372078316\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.97735742516525\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.48192490033472\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.90382831889973\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.36638003903653\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.21527952007494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.91286727780337\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.10521068172444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.12134757510746\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 160.10035385152875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.0515527768069\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.67694447040947\n",
      "Best: 0.235976 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 82.11219473479085\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 88.72053982507117\n",
      "Best: -172.392775 using {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 50.868410951114804\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.2395676617944\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 154.6773896519208\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65038633086903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.8757444623967\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.06935653054268\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.71046484778483\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.95609297090832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.66254950961613\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.310595651966\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 141.7319984478906\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.1328645737157\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.91881166260825\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.4228997361574\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.22298915376908\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 140.18070113929113\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.6445309213731\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.4062687831169\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 141.1834369654203\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.09960266763466\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 143.83970728591066\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26070279284443\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 147.61334188284422\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65025555929589\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 157.9432388428203\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.06923890257144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 158.34281142403844\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.95598847177354\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 163.21932775246765\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.3105042669022\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 172.57278782810792\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.1327862879574\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 186.4031916509593\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.4228345349392\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 204.71053922102163\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 140.18064900784756\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.49483053829513\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 143.40622970668244\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 254.75606560277964\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.09957663144394\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 286.49424441447525\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26068978213198\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 322.7093669733819\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 3: Mixed is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.17271774133052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.3879607615141\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.69097268110758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 195.64755090607935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.99283674898857\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.8027378041722\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.28537430373764\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 47.59245261528479\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08974406206201\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.18358393584879\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.285374632613\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 41.69371245963684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08979072382978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.27309822107458\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.285374632613\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 41.69371245963684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.42733805809158\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 222.82965723339348\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.285374632613\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 41.69371245963684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08979072382978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.27309822107458\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.285374632613\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 41.69371245963684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.42733805809158\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 222.85117805574092\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.285374632613\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 41.69371245963684\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.42733805809158\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 222.82510646940267\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 120.93463298540186\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 96.26176543247716\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.8157407981346\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.57917492624982\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.4877057651447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.0871581180312\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.45029786066212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 186.6831571432506\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.4877057651447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.0871581180312\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.45029786066212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 215.37193294754067\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.4877057651447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.0871581180312\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.45029786066212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 186.6831571432506\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.4877057651447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.0871581180312\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.3618495731795\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 202.93964247904782\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.4877057651447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.0871581180312\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.65736763471817\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 168.8176336313204\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 90.68034391462135\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 60.240436739439744\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.96952348453677\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 119.75294886406611\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 89.39951664555618\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 48.975956002102464\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.06305169937674\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 253.66785092488303\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 89.39951664555618\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 48.975956002102464\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.06305169937674\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 284.5693929459168\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 89.39951664555618\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 48.975956002102464\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.06305169937674\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 253.66785092488303\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 89.39951664555618\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 48.975956002102464\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.06305169937674\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.5016313072337\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 89.39951664555618\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 48.975956002102464\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.13624184827924\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 215.40973242746207\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.0950592038792\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.16410745113986\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 130.27466264711825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 105.15831792372765\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.15802164431832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.95966935765031\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.65502686582144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 99.7531604510019\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 101.58374210750732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.66661921983638\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.18371096921804\n",
      "Best: 0.167119 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 73.66006556085756\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 116.62851928139364\n",
      "Best: -142.802301 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 47.729136926124454\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 111.34111786350115\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.51212946414725\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08974406206201\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.18358393584879\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 109.24846063750049\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.6394207781935\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.90708405571418\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 115.15405780725418\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.06561431670305\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.72749502303081\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.72405142046712\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 107.35973242552342\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.88239536700637\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.05077001473194\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.5406461563208\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.80060779065647\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.69880378841043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.60924575329692\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.35686826327527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.47668390265335\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.51483958091531\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.40292223872572\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08979072382978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.27309822107458\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 109.39304707528288\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.6546997375867\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.20139432579661\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 136.83970443987303\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.514832475371\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 141.82811232793355\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.33336152400604\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.61992340176832\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.65698147170168\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 157.21513766137733\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.48569231845799\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 167.61375510676052\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.81949406427493\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 179.8157757379179\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.65838670915254\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 193.82119955484956\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.00237025309075\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 209.63002655755537\n",
      "\n",
      "Number of instances: 600\n",
      "****************************************************************************************************\n",
      "Scenario 1: Male is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73136473963345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.8480412515774\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51379907076607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.93399728006057\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.16693598803016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.41524325238066\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05733400122081\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 92.36742174941611\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.94524731098713\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.53961706477688\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05732693995382\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.06114746650077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.9452479801938\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.4405737617259\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05732693995382\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.06114746650077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.96737680420114\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 205.3356383094504\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05732693995382\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.06114746650077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.9452479801938\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.4405737617259\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05732693995382\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.06114746650077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.96737680420114\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 205.33563831186183\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.05732693995382\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.06114746650077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.96737680420114\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 205.3356383151029\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.28698423168674\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 95.12765917049207\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.7454232826042\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.81687756166342\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.36162295669978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.47907918201082\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31746501975508\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.73087321350147\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.36162295669978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.47907918201082\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31746501975508\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 259.8217048138785\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.36162295669978\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.47907918201082\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31746501975508\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.73087321350147\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.36162295669978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.47907918201082\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.46366145083958\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.81050785392492\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.36162295669978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.47907918201082\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31746501975508\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 185.1219022725769\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 128.2364027675259\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.07142607229673\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.48455963858693\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.9754807432215\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 127.74104700125527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.54763910169926\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.61532307402587\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.07630265383145\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 127.74104700125527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.54763910169926\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.61532307402587\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 269.9353288861402\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 127.74104700125527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.54763910169926\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.61532307402587\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.07630265383145\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 127.74104700125527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.54763910169926\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.14356598480754\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.13017961613093\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 127.74104700125527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.54763910169926\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.61532307402587\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 237.7847032583807\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.46927829631406\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.45572196939737\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.46317856489064\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.84477803542117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.2396417012065\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.17262553197014\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.9917519203369\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.2693354001331\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.68829476120067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.83573699045365\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06555563300707\n",
      "Best: 0.201138 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.49352915065496\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 104.13771339479615\n",
      "Best: -107.459044 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.06402377730012\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 119.20243154759308\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.57090673165591\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.94524731098713\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.53961706477688\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.8126255571851\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.92861704072122\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.74916680264236\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.86024867060688\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.75487104735888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.33451195443375\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.8297382913346\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.3514068922019\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.9737685345696\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.9109334839113\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.18696177706386\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.01309172956199\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.46931801881736\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.65788162915393\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.82083725983014\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.84530318268718\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24151950010216\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.57535639016164\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.9452479801938\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.4405737617259\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.93068470764126\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.4275341000241\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.97087694214943\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.0583442624037\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.06582468371833\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.3330042488648\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.21552793234797\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.25151405940733\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.41998668803832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.81387369403126\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.67920095078938\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 166.02008315273665\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.99317072060121\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 180.87014243552346\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.36189599747371\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 198.36405154239176\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.78537678140698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.50181047334144\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 2: Female is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874624\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 152.50434075622093\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.0711292058723\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.3153082208075\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08265961255316\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 72.06128538051877\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.84556212659383\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.30021517221982\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08246398768212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 65.737556443295\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.84541511513277\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.55869139158085\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08246398768212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 65.737556443295\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.86581026980105\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 250.3356736920796\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08246398768212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 65.737556443295\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.84541511513277\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.55869139158085\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08246398768212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 65.737556443295\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.86581026980105\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 250.33567369420416\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 97.08246398768212\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 65.737556443295\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.86581026980105\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 250.33567368749715\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.7087292875146\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.27148304230322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 152.0602857906405\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.7039350803502\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.31391944856588\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.22284561776186\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.67832025371305\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 281.7577424928733\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.31391944856588\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.22284561776186\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 152.1362059031276\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.54926575369996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.31391944856588\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.22284561776186\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.67832025371305\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 281.7577424928733\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.31391944856588\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.22284561776186\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 152.1362059031276\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 322.2979495747419\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.31391944856588\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.22284561776186\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 152.1362059031276\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 290.06285315505147\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281582\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 78.11846322137394\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.17138939290746\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.5564937088509\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 72.64179161686724\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.1713893929074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 329.8253010613458\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 72.64179161686724\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.1713893929074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 376.75673084347824\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 72.64179161686724\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.1713893929074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 329.8253010613458\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 72.64179161686724\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.1713893929074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 328.609292581162\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.73865835281578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 72.64179161686724\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.1713893929074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 306.31164119146484\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.9308852644264\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.62289205165081\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.90382831889973\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.36638003903653\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.21527952007494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.91286727780337\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.10521068172444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.12134757510746\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 160.10035385152875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.0515527768069\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.67694447040947\n",
      "Best: 0.235976 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 96.32641712935944\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 88.72053982507117\n",
      "Best: -172.392775 using {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 71.49242034367361\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.83204805964924\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.09571446810438\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.84556212659383\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.30021517221982\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.82703975402273\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.15293870295547\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.2136113982931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.88737090650545\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.00527705940482\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.50351178286974\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.202036737358\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.00136133204838\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.8038904321525\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.38091955404133\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.81083814378843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.6421864488486\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.2228798722658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 134.7851620164702\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.04001561758454\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.80984625690613\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26224537974468\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.7162391701564\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.84541511513277\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.55869139158085\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.82689385528903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.26239150942124\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.2134696319353\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 136.80621919858413\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.00514244507164\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.19017445906965\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.201912294698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 164.41425729087769\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.80377918081436\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 185.47846769400826\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.81074310342075\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 211.38280566846146\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.22280406251718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.12727121423717\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.0399620581036\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 277.71186433133545\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26221709018012\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 318.1365850197563\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 3: Mixed is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.17271774133052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.3879607615141\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.69097268110758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 195.64755090607935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.99283674898857\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.8027378041722\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.6109945873758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.43790434391052\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.79976924755259\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.72930405073458\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.61098597774215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 78.27823004398117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.79975962801433\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.3369485715784\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.61098597774215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 78.27823004398117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.83747303114566\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.87189080388862\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.61098597774215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 78.27823004398117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.79975962801433\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.3369485715784\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.61098597774215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 78.27823004398117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.83747303114566\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.87189080586194\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.61098597774215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 78.27823004398117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.83747303114566\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.87189080745063\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 120.7218918812541\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.46676089647264\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.34958003845426\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 107.1516164260589\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.78480596638006\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.80280857894697\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.66012611204538\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.7025606409116\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.78480596638006\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.80280857894697\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.66012611204538\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 225.5063945260556\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.78480596638006\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.80280857894697\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.66012611204538\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.7025606409116\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.78480596638006\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.80280857894697\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.66012611204538\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 198.21325634670683\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.78480596638006\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.80280857894697\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.27910301133436\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 168.1385203965962\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.45898196912432\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.80354101195203\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.10379973261037\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 114.91954807226527\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.91515893382224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.67206496470328\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.38528300999813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 270.7042164858308\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.91515893382224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.67206496470328\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.38528300999813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 275.0986461786052\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.91515893382224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.67206496470328\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.38528300999813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 270.7042164858308\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.91515893382224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.67206496470328\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.38528300999813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 271.26048425154704\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.91515893382224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.67206496470328\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.72831290436373\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 245.53900055443657\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 117.99554733562914\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.31908950120535\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 130.27466264711825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 105.15831792372765\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.15802164431832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.95966935765031\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.65502686582144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 99.7531604510019\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 101.58374210750732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.66661921983638\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.18371096921804\n",
      "Best: 0.167119 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.41502961046275\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 116.62851928139364\n",
      "Best: -142.802301 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.30138157463912\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.03581616988644\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.3593791696214\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.79976924755259\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.72930405073458\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.45205137069807\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.8478485077147\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.23433632189517\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.44357545671652\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.1466241011439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.5164848977401\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.18891470844426\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.06657683078542\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.36120814379622\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.09385125585251\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.66350440719984\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.59830817294136\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.09580349865507\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.57994758205191\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.65810541816192\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.03876948318423\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.3504101657204\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.97477387633828\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.79975962801433\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.3369485715784\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.60353492109965\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.03362191379513\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.54095315056767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 114.35115247272942\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.61201431641844\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 119.28954024838124\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.81671841865189\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.84878524075064\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.15506545726804\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.02888744983758\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.62705543226691\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.82984687564206\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.23268834364853\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 165.2516635181641\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.97196419141284\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 183.29433737740365\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.84488297555988\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 203.9578684533608\n",
      "\n",
      "Number of instances: 1100\n",
      "****************************************************************************************************\n",
      "Scenario 1: Male is target domain:\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73136473963345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.8480412515774\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51379907076607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.93399728006057\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.16693598803016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.41524325238066\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07637040644043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.40097333132736\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.78187995914436\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.80872379068774\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07636018595439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.96138834930503\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.7818783390445\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.86340339966952\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07636018595439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.96138834930503\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.79988592234169\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.507184197695\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07636018595439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.96138834930503\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.7818783390445\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.86340339966952\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07636018595439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.96138834930503\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.79988592234169\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.507184197695\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.07636018595439\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.96138834930503\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.79988592234169\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.507184197695\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.67912622582803\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 97.09893182235969\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74152880312009\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.39978023121965\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7105394936581\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.04944482563937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31061227992888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 217.56616459455535\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7105394936581\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.04944482563937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31061227992888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 256.7416082141329\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7105394936581\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.04944482563937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31061227992888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 217.56616459455535\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7105394936581\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.04944482563937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.39458118787971\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.65764537200954\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7105394936581\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.04944482563937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.31061227992888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 206.91312032013647\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.06508150298119\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 92.33392964720518\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.11902235334524\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.47700036415168\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.62460391983039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.50582098445932\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24248012417327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 212.94641786223914\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.62460391983039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.50582098445932\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24248012417327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 259.2307134246449\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.62460391983039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.50582098445932\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24248012417327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 212.94641786223914\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.62460391983039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.50582098445932\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.83123248737557\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 238.22074828877942\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.62460391983039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.50582098445932\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24248012417327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 243.20378093275883\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.6837968731225\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.3112131498363\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.46317856489064\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.84477803542117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.2396417012065\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.17262553197014\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.9917519203369\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.2693354001331\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.68829476120067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.83573699045365\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06555563300707\n",
      "Best: 0.201138 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 125.70893579080463\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 104.13771339479615\n",
      "Best: -107.459044 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.25905163874454\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.36359897690424\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.69333354907613\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.78187995914436\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.80872379068774\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.58121483471142\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.23542989329142\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.44624162194108\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.19040836111407\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.37696032083342\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.67365919415566\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.37337093138835\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.68518239241621\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.43547345360594\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.2249779558957\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.56326788748619\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.29304588459412\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.75675423302904\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.88938617851153\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.01593249023452\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.01399883764788\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.34080265910265\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.66688386200315\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.7818783390445\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.86340339966952\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.69824919471174\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.98538104266294\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.66613174319417\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 134.68958622785118\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.68552598449182\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 138.97601895523434\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.75643191860466\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.84467922481232\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.8788495455327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 155.2955670365852\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.05277886527594\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 167.32868239055293\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.2782198778344\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 181.94402528671557\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 122.55517258320803\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.14159572507307\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.8836369813969\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.92139370562535\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 2: Female is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874624\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 152.50434075622093\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.0711292058723\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.3153082208075\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79095868107902\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 78.92143088971821\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.59047440393745\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.79576211639811\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79076860882458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.8082957435208\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.59035023819327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.86082925025426\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79076860882458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.8082957435208\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.7072185660945\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 237.7735051083221\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79076860882458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.8082957435208\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.59035023819327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.86082925025426\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79076860882458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.8082957435208\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.7072185660945\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 237.7735051083221\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.79076860882458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 74.8082957435208\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.7072185660945\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 237.7735051083221\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55752420781192\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 97.17163137273235\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 149.04551107888813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 138.69407044793653\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.41965457672877\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.7044358299269\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 148.2689412210548\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 309.2125029496022\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.41965457672877\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.7044358299269\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 149.33665117529773\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 371.6142126891386\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.41965457672877\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.7044358299269\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 148.2689412210548\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 309.2125029496022\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.41965457672877\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.7044358299269\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 149.33665117529773\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 344.0589487269334\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.41965457672877\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.7044358299269\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 149.33665117529773\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 272.55209588940727\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 85.78835295144012\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177539\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.87203786723774\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.4439480242575\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177553\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 300.39078236225595\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.4439480242575\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177553\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 380.2611296088152\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.4439480242575\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177553\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 300.39078236225595\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.4439480242575\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177553\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 323.79355967358026\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.32549303463567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.4439480242575\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.74320447177553\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 289.20780597686246\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.21940031412242\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.4610046787441\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.90382831889973\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.36638003903653\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.21527952007494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.91286727780337\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.10521068172444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.12134757510746\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 160.10035385152875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.0515527768069\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.67694447040947\n",
      "Best: 0.235976 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 105.05205881308706\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 88.72053982507117\n",
      "Best: -172.392775 using {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 77.89682324398223\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.60605645395506\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.65394020121987\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.59047440393745\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.79576211639811\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.49641366158994\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.46228515148667\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.82990185653762\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.04088259299606\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.59093898878052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.53155444092626\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.7795250583186\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.9343006952773\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.39566006515187\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.24912135604917\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.43934400928035\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.47601642324187\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 142.91057689070402\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 134.6149858968554\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.80935870942292\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.66602977688976\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.13568946543697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.62914806334493\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.59035023819327\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.86082925025426\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.49629355259515\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.91346423888314\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.82978766203115\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.98863820846663\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.5908325665012\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.08635115900472\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.77942826600543\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 161.20660309049737\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.3955747605437\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.3493940029446\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.43927205011607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 208.51472389634648\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 142.91052013472253\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 239.70259277070286\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.80931901436313\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 275.91300062601385\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.1356686890378\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 317.1459474622794\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 3: Mixed is target domain:\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.17271774133052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.3879607615141\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.69097268110758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 195.64755090607935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.99283674898857\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.8027378041722\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76885952521768\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.05159222833713\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.9617620142965\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.69529732836072\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76880328071606\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 80.2259911338849\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.96170507758367\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.75203941155789\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76880328071606\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 80.2259911338849\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.02223841563486\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 173.6402915698044\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76880328071606\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 80.2259911338849\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.96170507758367\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.75203941155789\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76880328071606\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 80.2259911338849\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.02223841563486\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 173.6402915698044\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.76880328071606\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 80.2259911338849\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.02223841563486\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 173.6402915698044\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 119.94524154283889\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 98.32179780632399\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.91553247483232\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.01301649485086\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.23080875114404\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.13491236913129\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.11245522126417\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 205.19259232785254\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.23080875114404\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.13491236913129\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.11245522126417\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 216.91581996816186\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.23080875114404\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.13491236913129\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.11245522126417\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 205.19259232785254\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.23080875114404\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.13491236913129\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.11245522126417\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 215.2192539065838\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.23080875114404\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.13491236913129\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.86551354112636\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 173.42857424932188\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.68806435517149\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.46704342296104\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.94664705385986\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.16392626035666\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26379886864697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.04138065637449\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.32577691272043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 207.76022899092146\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26379886864697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.04138065637449\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.32577691272043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 245.90585363236755\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26379886864697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.04138065637449\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.32577691272043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 207.76022899092146\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26379886864697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.04138065637449\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.32577691272043\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 232.89522512857548\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26379886864697\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.04138065637449\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.69624823814469\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.79691667151837\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 116.22256041096074\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.15548651297863\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 130.27466264711825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 105.15831792372765\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.15802164431832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.95966935765031\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.65502686582144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 99.7531604510019\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 101.58374210750732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.66661921983638\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.18371096921804\n",
      "Best: 0.167119 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 108.83659630923509\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 116.62851928139364\n",
      "Best: -142.802301 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 84.48633296779128\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.66196946271091\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.6619148513564\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.9617620142965\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.69529732836072\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.7965700734383\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.9226165245943\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.73944202448271\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.62592397573492\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.7903778674297\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.80521968178262\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.94937760227921\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.46050364273738\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.21644122903133\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.59177585859918\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.59156874768601\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.19903632936806\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.07476015824328\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.28228505504397\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.66601546070311\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.84152203562697\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.36533465506552\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.87674727111698\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.96170507758367\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.75203941155789\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.96500233145458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.60841893136364\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.0762277426543\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 113.0853856097565\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.29538131118282\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.18293944673648\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.62246303704018\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.90108044230358\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.05747292022637\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 136.23980859645775\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.60041096074137\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.1991239091991\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.2512771585852\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 164.7790263805275\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.01007151375785\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.97951601044306\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.87679402625932\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 203.80059279894567\n",
      "\n",
      "Number of instances: 1600\n",
      "****************************************************************************************************\n",
      "Scenario 1: Male is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73136473963345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.8480412515774\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51379907076607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.93399728006057\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.16693598803016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.41524325238066\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.91609833552222\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.62603333119947\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35177612927238\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.94646074821262\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9160879563112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.91431397933943\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35177521083487\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.00711307525603\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9160879563112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.91431397933943\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.36103007687346\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 191.7711858480815\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9160879563112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.91431397933943\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35177521083487\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.00711307525603\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9160879563112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.91431397933943\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.36103007687346\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 191.7711858480815\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9160879563112\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 90.91431397933943\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.36103007687346\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 191.7711858480815\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8449107911294\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 98.46786810785068\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73593563914412\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.34856970263249\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.88060146678262\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.73329159457549\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26435390456372\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 247.33602916081358\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.88060146678262\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 86.73329159457549\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26435390456372\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 246.45625210414525\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.88060146678262\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.73329159457549\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26435390456372\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 247.33602916081358\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.88060146678262\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.73329159457549\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35297458796272\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 194.74731790858615\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.88060146678262\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.73329159457549\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26435390456372\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 202.5390311931815\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.11880450398404\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 92.32730439222146\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.64479086148954\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.887925521769\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.73406535702048\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.03490299992413\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74394488768719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 233.6640189912056\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.73406535702048\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.03490299992413\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74394488768719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 244.11753648389094\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.73406535702048\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.03490299992413\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74394488768719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 233.6640189912056\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.73406535702048\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.03490299992413\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.3994482924392\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 231.25535388424976\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.73406535702048\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.03490299992413\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.74394488768719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 212.0290964945991\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.14620912601133\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.2370570761972\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.46317856489064\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.84477803542117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.2396417012065\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.17262553197014\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.9917519203369\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.2693354001331\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.68829476120067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.83573699045365\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06555563300707\n",
      "Best: 0.201138 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.34731051000281\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 104.13771339479615\n",
      "Best: -107.459044 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.13892624108627\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.02084974385902\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.09585550185926\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35177612927238\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.94646074821262\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.13391848136267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.95521277697232\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.97290894655208\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.4042772549713\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.8687475248405\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.29365418220961\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.82143421622801\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.62334355868715\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.83096902071458\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.393345384404\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.89735193830022\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.60365965936009\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.02058296898493\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.2542863835555\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.2006621127687\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.34522555699019\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.43758936965153\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.87647717966415\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35177521083487\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.00711307525603\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.24533774156079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.30016599079832\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.18281610682686\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.1786806174005\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.1642103066331\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.6426569550625\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.18952034097947\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.69209500378435\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.25874620986599\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 154.32699476356606\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.37188791329267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 166.5473562344077\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.52894545125952\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 181.3531794163091\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.7299188237665\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 198.74446430927043\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.97480803081366\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.7212109132916\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Scenario 2: Female is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874624\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 152.50434075622093\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.0711292058723\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.3153082208075\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75227666906495\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 85.40113135920204\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.86111775644173\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.69557236306659\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75218215636389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.74853369778333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.860887020628\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.24615381014678\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75218215636389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.74853369778333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.86578499794267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 233.59781857701523\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75218215636389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.74853369778333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.860887020628\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.24615381014678\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75218215636389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.74853369778333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.86578499794267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 233.59781857701523\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.75218215636389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.74853369778333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.86578499794267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 233.59781857701523\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.32120328368228\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 100.98401836814985\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.1089307594397\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.91640267349857\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.72686467094013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.48936329261768\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.19749257441893\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 319.20492137372094\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.72686467094013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 89.48936329261768\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.679681116744\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 371.3263870243454\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.72686467094013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.48936329261768\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.19749257441893\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 319.20492137372094\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.72686467094013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.48936329261768\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.679681116744\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 336.28618926874447\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.72686467094013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.48936329261768\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.679681116744\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 298.42989786884533\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410142\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 90.34116336981769\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922135\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.52613629082217\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.1432606715367\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 287.60423128622085\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.1432606715367\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 376.6191128451651\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.1432606715367\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 287.60423128622085\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.1432606715367\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 352.731961340876\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.30117883410134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.1432606715367\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.27946181922178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 304.68731958476525\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.94153334879152\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.43438339240203\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.90382831889973\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.36638003903653\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.21527952007494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.91286727780337\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.10521068172444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.12134757510746\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 160.10035385152875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.0515527768069\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.67694447040947\n",
      "Best: 0.235976 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09001962029832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 88.72053982507117\n",
      "Best: -172.392775 using {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.60258422191937\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.29618439543442\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.40669204416173\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.86111775644173\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.69557236306659\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.0435899841886\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.24644286761435\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.60836730359847\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.60398144655494\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.55544971467143\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.7681880998883\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.88483721740735\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.73906282761453\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.59652981180628\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.51660562973355\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.69052749786823\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.1008165062454\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.1668302755932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 134.49169545715006\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.02543814498122\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.68924248244753\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26635110603223\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.69345758213782\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.860887020628\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.24615381014678\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.04337890069704\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.59535320497339\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.60817663270888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.73707005582415\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.55528021666362\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.67130436269903\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.8846896525612\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.3980561255981\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.5964049404016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 183.91732534452134\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.6904260801849\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.2291120194687\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.16675307191105\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.33341615044017\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.02538591558002\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 277.23023773743586\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.26632461119186\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 317.91957678045566\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 3: Mixed is target domain:\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.17271774133052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.3879607615141\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.69097268110758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 195.64755090607935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.99283674898857\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.8027378041722\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09395972137952\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.16027009255497\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.00473605046406\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.0217246971762\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09390448973805\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.30195554279155\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.0046878216568\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.49024556844765\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09390448973805\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.30195554279155\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.03599713106551\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.30241137813607\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09390448973805\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.30195554279155\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.0046878216568\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.49024556844765\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09390448973805\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.30195554279155\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.03599713106551\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.30241137813607\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09390448973805\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.30195554279155\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.03599713106551\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.30241137813607\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 119.14427794829521\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 98.62850810924463\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.7570540753436\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.05312751026565\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55191906219656\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.4965843457266\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.938215368517\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 216.40685096992536\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55191906219656\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.4965843457266\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.938215368517\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 238.2975329159741\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55191906219656\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.4965843457266\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.938215368517\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 216.40685096992536\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55191906219656\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.4965843457266\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.938215368517\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.05081577147607\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.55191906219656\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.4965843457266\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.65774350411039\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 179.74104800263078\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.14289969051195\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.85716183963085\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.67661508167579\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.26101072052374\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.79012498633932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.10865942247457\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.04878133570502\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.45773579920714\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.79012498633932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.10865942247457\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.04878133570502\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 238.9451204254295\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.79012498633932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.10865942247457\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.04878133570502\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 199.45773579920714\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.79012498633932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.10865942247457\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.04878133570502\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 203.72939480526802\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.79012498633932\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.10865942247457\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.45939163044062\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 197.8874876316491\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 114.3188497703744\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.496255034075\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 130.27466264711825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 105.15831792372765\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.15802164431832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.95966935765031\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.65502686582144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 99.7531604510019\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 101.58374210750732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.66661921983638\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.18371096921804\n",
      "Best: 0.167119 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 108.22009296400691\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 116.62851928139364\n",
      "Best: -142.802301 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.16576084312739\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.20676309395246\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.9503986328722\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.00473605046406\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.0217246971762\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.74705682379104\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.67862325559376\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.59481701839793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.75323849134828\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.54801663428475\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.24557040443973\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.60665567145152\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.15561899486812\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.77073412989819\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.48338426263346\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.04025200962481\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.22886620773572\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.41520931063134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.39206483017492\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.8956060329178\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.97298012995103\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.48144217648418\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.97161210706409\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.0046878216568\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.49024556844765\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.94537445262954\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 107.33876259685633\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.9847253183735\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.68209831178724\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.12274041888871\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 116.52025271324041\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.35941975417512\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.8532258012158\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.6947633242328\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.68101757571347\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.1287711290617\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.00362803673337\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.66144316866183\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 164.82105718427553\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test: 106.2927794430332\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 183.1333050183399\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.02277995217578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 203.94037153892654\n",
      "\n",
      "Number of instances: 2100\n",
      "****************************************************************************************************\n",
      "Scenario 1: Male is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.73136473963345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.8480412515774\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.26361307240093\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.28341922837257\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51379907076607\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.93399728006057\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.5779168018816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.89691438365648\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.16693598803016\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 175.41524325238066\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.3391095779829\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 96.1507939097195\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.75363367007692\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.73470034124067\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.33909868642843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.11384005455297\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.75363494041859\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.37276185931887\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.33909868642843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.11384005455297\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.80164055538913\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 189.20264487447318\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.33909868642843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.11384005455297\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.75363494041859\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.37276185931887\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.33909868642843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.11384005455297\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.80164055538913\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 189.20264487447318\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.33909868642843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 93.11384005455297\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.80164055538913\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 189.20264487447318\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.61073942983671\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 98.33847481051077\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.79203787673067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.45665764477138\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.61942721567343\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.77743831658418\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.28584584678737\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.4161947632566\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.61942721567343\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.77743831658418\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.28584584678737\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 267.4486291187626\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.61942721567343\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.77743831658418\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.28584584678737\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.4161947632566\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.61942721567343\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.77743831658418\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.35344299219939\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 225.88917691012935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.61942721567343\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.77743831658418\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.28584584678737\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 245.39914403173697\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.9392666602552\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 94.13462861814011\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.89705631367157\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.387420842845\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.54927968361818\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.60923420103322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.98172200421561\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 248.46494393938096\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.54927968361818\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.60923420103322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.98172200421561\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 287.34721564533703\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.54927968361818\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.60923420103322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.98172200421561\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 248.46494393938096\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.54927968361818\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.60923420103322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.68292364404174\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 239.1460903675639\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06801454617023\n",
      "Best: 0.204356 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.54927968361818\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 119.67222406584169\n",
      "Best: -114.305620 using {'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.60923420103322\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.98172200421561\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 267.8042867360176\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.587841323962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.19097930564791\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.24079158271992\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.02197655014766\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.46317856489064\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 90.84477803542117\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 124.2396417012065\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.17262553197014\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.9917519203369\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.2693354001331\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.68829476120067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 112.83573699045365\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06555563300707\n",
      "Best: 0.201138 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.6034351455859\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 104.13771339479615\n",
      "Best: -107.459044 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 95.74412733289942\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.576548912611\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.27600760077783\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.75363367007692\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.73470034124067\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.50869272166626\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.78113487774503\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.31768823000365\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.26421375970034\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.18062019508912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.1839369871066\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.09748861692267\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.54030455996383\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.06829349550428\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.33331647827201\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.09303483083397\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.56297274203116\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.17171262291174\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.22927335124128\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.30432687173756\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.33221830590236\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.49087757731147\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.87180760601441\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.75363494041859\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.37276185931887\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.6157338750248\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.7102751808247\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.51981033820701\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.63746681686376\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.46586432996521\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.15433676743604\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.45389585029943\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.26088503254155\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.48390489920966\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.95711161218028\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.55589147669592\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 166.24301650635226\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.66985558275813\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 181.11859971505754\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.82579721739641\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 198.583861238296\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.02371638061068\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 218.63880107606764\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 2: Female is target domain:\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874624\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 152.50434075622093\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 363.4014332794996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 359.0711292058723\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.83309899827903\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.32642415582451\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.88956915874658\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.3153082208075\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11118595463803\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 84.11948544940873\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57249545216281\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.1216937798991\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11106037460051\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.83218164609949\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57239830268334\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.97186054258835\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11106037460051\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.83218164609949\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57658487371104\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.43163910097735\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11106037460051\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.83218164609949\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57239830268334\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.97186054258835\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11106037460051\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.83218164609949\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57658487371104\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.43163910097735\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.11106037460051\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.83218164609949\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57658487371104\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 242.43163910097735\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.2797499056764\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 98.16255896713409\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 145.97860460383134\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.41069733332964\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.48432037617812\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.5430821039526\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 144.9399595900929\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 295.3546498600191\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.48432037617812\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.5430821039526\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.7028555489817\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 369.29966252979455\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.48432037617812\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.5430821039526\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 144.9399595900929\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 295.3546498600191\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.48432037617812\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.5430821039526\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.7028555489817\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 361.10027264911866\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.48432037617812\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.5430821039526\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.7028555489817\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 284.8648737304306\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.1038672662272\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.75942958308264\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401167\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.50901534519673\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.10386726622718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.71924738344124\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 283.6258219527349\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.10386726622718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.71924738344124\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 380.8600725673158\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.10386726622718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.71924738344124\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 283.6258219527349\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.10386726622718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.71924738344124\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 303.57907274699977\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36810817340125\n",
      "Best: 0.234641 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.10386726622718\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 83.18531873397544\n",
      "Best: -180.899357 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.71924738344124\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.44154046401079\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 298.7859749641995\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.8330989982791\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57359923745553\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.572182136704\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.29667695165327\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.90382831889973\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.36638003903653\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.21527952007494\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.91286727780337\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.10521068172444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 89.12134757510746\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 160.10035385152875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.0515527768069\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.67694447040947\n",
      "Best: 0.235976 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.42594795487558\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 88.72053982507117\n",
      "Best: -172.392775 using {'alpha': 0.01, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.37075507069832\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.02795073619319\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.0364104568394\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57249545216281\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.1216937798991\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.91821093986766\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.82175825969065\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.59414684600662\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.26364501011345\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.6003031705797\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.44735403116746\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.9366799135869\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.3728853228528\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.60327707502816\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.04023888516934\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 140.6000946549036\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.44941471811717\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.92713265321308\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.6004128216962\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.5843910699567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 140.49323319590656\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.5718699051344\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 146.12787584074812\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.57239830268334\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.97186054258835\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.91811585678522\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.8837406390394\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.59405552899918\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.49141566376602\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.60021731932525\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 145.79488561676823\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.93660122776345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.794150498046\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.60320725431373\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 184.48921030759934\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 140.6000353989761\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 210.88006504542824\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.92708566175057\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 241.96671471153275\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.58435804263712\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 277.74915930591277\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.57185254163582\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 318.22739882856837\n",
      "\n",
      "****************************************************************************************************\n",
      "Scenario 3: Mixed is target domain:\n",
      "****************************************************************************************************\n",
      "____________________ Scenario 1: SRCONLY ____________________\n",
      "* SRCONLY Results *\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.17271774133052\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.3879607615141\n",
      "\n",
      "* SRCONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.85144469608962\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 227.24225674603542\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.69097268110758\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 195.64755090607935\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.00322917595912\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 91.52290676649376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.99283674898857\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 162.8027378041722\n",
      "\n",
      "____________________ Scenario 1: TGTONLY ____________________\n",
      "* TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09644257048708\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.12349506447657\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.14705817471145\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.80770438909059\n",
      "\n",
      "* TGTONLY FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09641511298793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.59764107395125\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.14703191289124\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.93708667818491\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09641511298793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.59764107395125\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.16254307426823\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 171.18542357033877\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09641511298793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.59764107395125\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.14703191289124\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.93708667818491\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09641511298793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.59764107395125\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.16254307426823\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 171.18542357033877\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 109.09641511298793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 83.59764107395125\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.16254307426823\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 171.18542357033877\n",
      "\n",
      "____________________ Scenario 1: ALL ____________________\n",
      "* ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.56280756568206\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.89565102091649\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.59900499517245\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.37139547326323\n",
      "\n",
      "* ALL FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.03245356972454\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.08376243398745\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.77093374466891\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.94534735411483\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.03245356972454\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.08376243398745\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.77093374466891\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 240.3014045946741\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.03245356972454\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.08376243398745\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.77093374466891\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 224.94534735411483\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.03245356972454\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.08376243398745\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.77093374466891\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 197.6718462274462\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.03245356972454\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 88.08376243398745\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.43606579162217\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 192.29347229769309\n",
      "\n",
      "____________________ Scenario 1: WEIGHTED ____________________\n",
      "* WEIGHTED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.34656559120103\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 89.08708315411157\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.0785358117801\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.76837632541474\n",
      "\n",
      "* WEIGHTED FEDA Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.8782710970698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.08203903023647\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.49034247503936\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 225.37293561460996\n",
      "\n",
      "Checking hyperparameters\n",
      "General\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.8782710970698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.08203903023647\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.49034247503936\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 261.94189940628553\n",
      "\n",
      "Target\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.8782710970698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.08203903023647\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.49034247503936\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 225.37293561460996\n",
      "\n",
      "Domain 1\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.8782710970698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.08203903023647\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.49034247503936\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 219.68305336879894\n",
      "\n",
      "Domain 2\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24902408451032\n",
      "Best: 0.157140 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.8782710970698\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 136.26758715252544\n",
      "Best: -124.900134 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 100, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 85.08203903023647\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.82218443660192\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 232.96327338645054\n",
      "\n",
      "____________________ Scenario 1: PRED ____________________\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.42839905708067\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.10631555413285\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 114.11663440818647\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 116.43374744207144\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 130.27466264711825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 105.15831792372765\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 110.15802164431832\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 117.95966935765031\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.65502686582144\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 99.7531604510019\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 101.58374210750732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.66661921983638\n",
      "\n",
      "* PRED Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.18371096921804\n",
      "Best: 0.167119 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 108.19903687834139\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 116.62851928139364\n",
      "Best: -142.802301 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.09627106941485\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.80269287124621\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.83261274199363\n",
      "\n",
      "____________________ Scenario 1: LININT ____________________\n",
      "* LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.14705817471145\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.80770438909059\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.71250542783413\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.62574485780826\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.39731239285443\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 100.85267091953142\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.20147906977233\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 100.48848257426008\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.12500545858782\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 100.5331798219942\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.16789155930094\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 100.9867626627338\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.33013737191163\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.8492310964789\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.61174289641997\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.12058512322947\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.01270813282586\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.80082474298553\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.53303308112939\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.88994995574707\n",
      "\n",
      "* LININT FEDA Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.14703191289124\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.93708667818491\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.93248945891378\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.80249095661527\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.82572116766906\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.17125361912447\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.82672703915706\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 116.04337466571248\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.9355070733778\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.4188540963793\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.15206127033127\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.29769191112493\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.47638963001748\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.67988810994942\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.90849215243641\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 164.56544269285268\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.44836883758809\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 182.9543556598348\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 107.09601968547248\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 203.84662701089567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def FEDA(source_1, source_2, target, scale = 1):\n",
    "    target = np.c_[target, target, np.zeros(target.shape), np.zeros(target.shape)]\n",
    "    source_1 = np.c_[source_1, np.zeros(source_1.shape), source_1, np.zeros(source_1.shape)]\n",
    "    source_2 = np.c_[source_2 , np.zeros(source_2.shape), np.zeros(source_2.shape), source_2]\n",
    "    source = np.r_[source_1, source_2]\n",
    "    return source, target\n",
    "\n",
    "## Hyperparameter Sensitivity, features\n",
    "def check_hyperparameter_behaviour(X_train,y_train, X_test, y_test, X_dev, y_dev):\n",
    "    print('General')\n",
    "    X_general_test = np.c_[X_test, np.zeros(X_test.shape), np.zeros(X_test.shape), np.zeros(X_test.shape)]\n",
    "    get_predictions_from_baseline(X_train,y_train, X_general_test, y_test, X_dev, y_dev)\n",
    "    print('Target')\n",
    "    X_target_test = np.c_[X_test, X_test, np.zeros(X_test.shape), np.zeros(X_test.shape)]\n",
    "    get_predictions_from_baseline(X_train,y_train, X_target_test, y_test, X_dev, y_dev)\n",
    "    print('Domain 1')\n",
    "    X_domain_1_test = np.c_[X_test, np.zeros(X_test.shape), X_test, np.zeros(X_test.shape)]\n",
    "    get_predictions_from_baseline(X_train,y_train, X_domain_1_test, y_test, X_dev, y_dev)\n",
    "    print('Domain 2')\n",
    "    X_domain_2_test = np.c_[X_test, np.zeros(X_test.shape), np.zeros(X_test.shape), X_test]\n",
    "    get_predictions_from_baseline(X_train,y_train, X_domain_2_test, y_test, X_dev, y_dev)\n",
    "\n",
    "def get_predictions_from_baseline(X_train,y_train,  X_test, y_test, X_dev, y_dev):\n",
    "    y_pred_lin = evaluate_on_linear_model(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    y_pred_nn = evaluate_on_neural_network(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    get_mse(y_pred_lin, y_pred_nn, y_test)  \n",
    "    return y_pred_lin, y_pred_nn\n",
    "    \n",
    "def downsampling_for_weight(X_source, y_source, X_target, y_target):\n",
    "    target_len = len(X_target)\n",
    "    source = pd.concat([X_source, y_source], axis = 1)    \n",
    "    downsampled_source = resample(source, n_samples = target_len, random_state = 99)\n",
    "    downsampled_source_y = downsampled_source['Exam Score']\n",
    "    downsampled_source_X = downsampled_source.drop(['Exam Score'], axis=1)\n",
    "    X = pd.concat([downsampled_source_X, X_target]) \n",
    "    y = pd.concat([downsampled_source_y, y_target])\n",
    "    return X, y\n",
    "\n",
    "def find_best_weight(src_predictions_1, tgt_predictions_1, src_predictions_2, tgt_predictions_2, y_test):\n",
    "    src_predictions_1 = pd.DataFrame(src_predictions_1)\n",
    "    tgt_predictions_1 = pd.DataFrame(tgt_predictions_1 )\n",
    "    src_predictions_2 = pd.DataFrame(src_predictions_2)\n",
    "    tgt_predictions_2 = pd.DataFrame(tgt_predictions_2)\n",
    "    \n",
    "    for i in np.arange(0, 1, 0.1):\n",
    "        print('Weight: ', i, 1 - i)\n",
    "        # linear model\n",
    "        new_src_predictions_1 = src_predictions_1 * i\n",
    "        new_tgt_predictions_1 = tgt_predictions_1 * (1 - i)\n",
    "        predictions_1 = new_src_predictions_1 + new_tgt_predictions_1\n",
    "        # Neural network\n",
    "        new_src_predictions_2 = src_predictions_2 * i\n",
    "        new_tgt_predictions_2 = tgt_predictions_2 * (1 - i)\n",
    "        predictions_2 = new_src_predictions_2 + new_tgt_predictions_2 \n",
    "        get_mse(predictions_1, predictions_2, y_test)\n",
    "        \n",
    "def evaluate_baselines(target, instance_size):\n",
    "    \n",
    "    dev_instance_size = 100\n",
    "    domains = ['male', 'female', 'mixed']\n",
    "    domains.remove(target)\n",
    "    source = domains\n",
    "    X_source_1_train, y_source_1_train, X_source_1_test, y_source_1_test, X_source_1_dev, y_source_1_dev = create_domains(source[0])\n",
    "    X_source_2_train, y_source_2_train, X_source_2_test, y_source_2_test, X_source_2_dev, y_source_2_dev = create_domains(source[1])\n",
    "\n",
    "    print('_'*20, 'Scenario 1: SRCONLY','_'*20)  \n",
    "    print('* SRCONLY Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    X_train = pd.concat([X_source_1_train, X_source_2_train])\n",
    "    y_train = pd.concat([y_source_1_train, y_source_2_train])\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    y_srconly_pred_lin, y_srconly_pred_nn = get_predictions_from_baseline(X_train,y_train, X_test, y_test, X_dev, y_dev)\n",
    "    print('* SRCONLY FEDA Results *')\n",
    "    X_source_train, X_target_train = FEDA(X_source_1_train, X_source_2_train, X_target_train[:instance_size])\n",
    "    X_source_test, X_target_test = FEDA(X_source_1_test, X_source_2_test, X_target_test)\n",
    "    X_source_dev, X_target_dev = FEDA(X_source_1_dev, X_source_2_dev, X_target_dev[:dev_instance_size])\n",
    "    y_srconly_feda_pred_lin, y_srconly_feda_pred_nn = get_predictions_from_baseline(X_source_train,y_train, X_target_test, y_test, X_target_dev, y_dev)\n",
    "    print('Checking hyperparameters')\n",
    "    check_hyperparameter_behaviour(X_source_train,y_train, X_test, y_test, X_target_dev, y_dev)\n",
    "\n",
    "    print('_'*20, 'Scenario 1: TGTONLY','_'*20)\n",
    "    print('* TGTONLY Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    X_train = X_target_train[:instance_size]\n",
    "    y_train = y_target_train[:instance_size]\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    y_tgtonly_pred_lin, y_tgtonly_pred_nn = get_predictions_from_baseline(X_train,y_train, X_test, y_test, X_dev, y_dev)\n",
    "    print('* TGTONLY FEDA Results *')\n",
    "    X_source_train, X_target_train = FEDA(X_source_1_train, X_source_2_train, X_target_train[:instance_size])\n",
    "    X_source_test, X_target_test = FEDA(X_source_1_test, X_source_2_test, X_target_test)\n",
    "    X_source_dev, X_target_dev = FEDA(X_source_1_dev, X_source_2_dev, X_target_dev[:dev_instance_size])\n",
    "    y_tgtonly_feda_pred_lin, y_tgtonly_feda_pred_nn = get_predictions_from_baseline(X_target_train, y_train, X_target_test, y_test, X_target_dev, y_dev)\n",
    "    print('Checking hyperparameters')\n",
    "    check_hyperparameter_behaviour(X_target_train,y_train, X_test, y_test, X_target_dev, y_dev)\n",
    "\n",
    "    print('_'*20, 'Scenario 1: ALL','_'*20)\n",
    "    print('* ALL Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    X_train = pd.concat([X_source_1_train, X_source_2_train, X_target_train[:instance_size]])\n",
    "    y_train = pd.concat([y_source_1_train, y_source_2_train, y_target_train[:instance_size]])\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    get_predictions_from_baseline(X_train,y_train, X_test, y_test, X_dev, y_dev)\n",
    "    print('* ALL FEDA Results *')\n",
    "    X_source_train, X_target_train = FEDA(X_source_1_train, X_source_2_train, X_target_train[:instance_size])\n",
    "    X_source_test, X_target_test = FEDA(X_source_1_test, X_source_2_test, X_target_test)\n",
    "    X_source_dev, X_target_dev = FEDA(X_source_1_dev, X_source_2_dev, X_target_dev[:dev_instance_size])\n",
    "    X_train = np.r_[X_source_train, X_target_train]\n",
    "    get_predictions_from_baseline(X_train, y_train, X_target_test, y_test, X_target_dev, y_dev)\n",
    "    print('Checking hyperparameters')\n",
    "    check_hyperparameter_behaviour(X_train,y_train, X_test, y_test, X_target_dev, y_dev)\n",
    "\n",
    "    print('_'*20, 'Scenario 1: WEIGHTED','_'*20)\n",
    "    print('* WEIGHTED Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    X_target = X_target_train[:instance_size]\n",
    "    y_target = y_target_train[:instance_size]\n",
    "    X_1, y_1 = downsampled_X_source_1 = downsampling_for_weight(X_source_1_train, y_source_1_train, X_target, y_target)\n",
    "    X_2, y_2 = downsampled_X_source_2 = downsampling_for_weight(X_source_2_train, y_source_2_train, X_target, y_target)\n",
    "    X_train = pd.concat([X_1, X_2, X_target]) \n",
    "    y_train = pd.concat([y_1, y_2, y_target])\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    get_predictions_from_baseline(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    print('* WEIGHTED FEDA Results *')\n",
    "    X_source_train, X_target_train = FEDA(X_1, X_2, X_target_train[:instance_size])\n",
    "    X_source_test, X_target_test = FEDA(X_source_1_test, X_source_2_test, X_target_test)\n",
    "    X_source_dev, X_target_dev = FEDA(X_source_1_dev, X_source_2_dev, X_target_dev[:dev_instance_size])\n",
    "    X_train = np.r_[X_source_train, X_target_train]\n",
    "    get_predictions_from_baseline(X_train, y_train, X_target_test, y_test, X_target_dev, y_dev)\n",
    "    print('Checking hyperparameters')\n",
    "    check_hyperparameter_behaviour(X_train,y_train, X_test, y_test, X_target_dev, y_dev)\n",
    "\n",
    "    print('_'*20, 'Scenario 1: PRED','_'*20)\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "\n",
    "    # SRCONLY predictions\n",
    "    # Training data\n",
    "    X_train = pd.concat([X_source_1_train, X_source_2_train])\n",
    "    y_train = pd.concat([y_source_1_train, y_source_2_train])\n",
    "    X_test = X_target_train[:instance_size]\n",
    "    y_test = y_target_train[:instance_size]\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "\n",
    "    predictions_train_1 , predictions_train_2 = get_predictions_from_baseline(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    X_train = X_target_train[:instance_size]\n",
    "    df_train_final_1 = add_prediction_features(X_train, predictions_train_1)\n",
    "    df_train_final_2 = add_prediction_features(X_train, predictions_train_2)\n",
    "\n",
    "    # Testing data\n",
    "    X_train = pd.concat([X_source_1_test, X_source_2_test])\n",
    "    y_train = pd.concat([y_source_1_test, y_source_2_test])\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    predictions_test_1 , predictions_test_2 = get_predictions_from_baseline(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    df_test_final_1 = add_prediction_features(X_target_test, predictions_test_1)\n",
    "    df_test_final_2 = add_prediction_features(X_target_test, predictions_test_2)\n",
    "\n",
    "    # Development data\n",
    "    X_train = pd.concat([X_source_1_dev, X_source_2_dev])\n",
    "    y_train = pd.concat([y_source_1_dev, y_source_2_dev])\n",
    "    X_test = X_target_dev[:dev_instance_size]\n",
    "    y_test = y_target_dev[:dev_instance_size]\n",
    "    predictions_dev_1 , predictions_dev_2 = get_predictions_from_baseline(X_train, y_train, X_test, y_test, X_dev, y_dev)\n",
    "    X_target_dev = X_target_dev[:dev_instance_size]\n",
    "    df_dev_final_1 = add_prediction_features(X_target_dev,predictions_dev_1)\n",
    "    df_dev_final_2 = add_prediction_features(X_target_dev,predictions_dev_2)\n",
    "\n",
    "    y_train = y_target_train[:instance_size]\n",
    "    y_test = y_target_test\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "\n",
    "    print('* PRED Results *')\n",
    "    y_test_pred_lin = evaluate_on_linear_model(df_train_final_1, y_train, df_test_final_1, y_test, df_dev_final_1, y_dev)\n",
    "    y_test_pred_nn = evaluate_on_neural_network(df_train_final_2, y_train, df_test_final_2, y_test, df_dev_final_2, y_dev)\n",
    "    get_mse(y_test_pred_lin, y_test_pred_nn, y_test)\n",
    "\n",
    "    print('_'*20, 'Scenario 1: LININT','_'*20)\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    print('* LININT Results *')\n",
    "    find_best_weight(y_srconly_pred_lin, y_tgtonly_pred_lin, y_srconly_pred_nn, y_tgtonly_pred_nn, y_test)\n",
    "    print('* LININT FEDA Results *')\n",
    "    find_best_weight(y_srconly_feda_pred_lin, y_tgtonly_feda_pred_lin, y_srconly_feda_pred_nn, y_tgtonly_feda_pred_nn, y_test)\n",
    "\n",
    "def run_scenarios():\n",
    "    n_instances = [100, 600, 1100, 1600, 2100]\n",
    "    for instances in n_instances:\n",
    "        print('Number of instances:', instances)\n",
    "        print('*'*100)\n",
    "        print('Scenario 1: Male is target domain:')\n",
    "        print('*'*100)\n",
    "        evaluate_baselines('male',instances)\n",
    "        print('*'*100)\n",
    "        print('Scenario 2: Female is target domain:')\n",
    "        print('*'*100)\n",
    "        evaluate_baselines('female',instances)\n",
    "        print('*'*100)\n",
    "        print('Scenario 3: Mixed is target domain:')\n",
    "        print('*'*100)\n",
    "        evaluate_baselines('mixed', instances)\n",
    "        \n",
    "run_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return of Frustratingly Easy Domain Adaptation: CORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "CORAL\n",
      "****************************************************************************************************\n",
      "Domain: male\n",
      "\n",
      "Number of instances: 100\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.89954432324721\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 92.82433443531427\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.59874317030716\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.56011004990154\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.1367248438925\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 74.20861306916635\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.049457670542\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.59020507474003\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.46690489319873\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.18720537818668\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.59446297353215\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.38627839020108\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.86214369106159\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 92.91672542941785\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.5125593671203\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.74932093162505\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.049457670542\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 149.59020507474003\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 126.96477339586389\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 142.69876961185665\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 125.22222530444232\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 136.91587325128424\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.82181339627738\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.24151599302286\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.76353767136902\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.6756978370725\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.04739812971724\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.21841878343315\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.67339477132202\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.86967883210481\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.64152759618345\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.62947798308748\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.95179660430145\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.49781623638113\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.604201795676\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.47469359198583\n",
      "\n",
      "Number of instances: 300\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.66422904053678\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.02485604880471\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.01493900826614\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.26910199477169\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.08751086833217\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 87.12369288691623\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.43133148695463\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.03349728810754\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.38802913219149\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.91187410939742\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 123.00793394306396\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.95646936787297\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 124.77388032658705\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 94.0950051285655\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.95887211206909\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.51435125165605\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.43133148695463\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.03349728810754\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.08586883508511\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.45786095800428\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.83014471743792\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.45982391696096\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.66415913401315\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.03938616497757\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.58791208481071\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.19654770205408\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.6014035698307\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 119.93130852819054\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.70463358907301\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.24366864338691\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.89760214253775\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.1336280476432\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.18030923022482\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.60118674095945\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.55275485213433\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.64634472333559\n",
      "\n",
      "Number of instances: 500\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.57343184986753\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.32951876090044\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.78942299765907\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.58975901677127\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 119.63566553553737\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 87.70068132291333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.12469902322103\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.75658328491136\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.76895948562313\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 94.34248670664607\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.77840821885493\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.62326441652407\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 123.58628590114246\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.30961034666089\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.77849517037261\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.92049195764525\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.12469902322103\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 130.75658328491136\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.00062085693418\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.84053352860833\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.94110948258745\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.479898734414\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.94616490018092\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.6746789023283\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.01578710971447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.42487403235128\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.14997611118821\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.73048412448294\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.3487319046021\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.59150917872326\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.61205448995612\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.00794919507227\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.93994386725032\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.97980417352991\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.3324000364846\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.50707411409626\n",
      "\n",
      "Number of instances: 700\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 112.45705037957673\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.03201084541246\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.85047405851427\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.24256012150305\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 122.33291319010294\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 91.38559901318415\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.89826802828719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.37113171909758\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 114.41461810371625\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 94.78802940061924\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.85274332189468\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.96560489543636\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 71.06837724894115\n",
      "Best: 0.204351 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 126.08610386355443\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 86.71825093652642\n",
      "Best: -115.878823 using {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "MSE Train: 93.6423282580846\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.76801030038861\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.4668950586554\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.89826802828719\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.37113171909758\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.78044944041721\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.5413098376413\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.73219511719003\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.24859122767323\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.75350505860567\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.49297588919332\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 120.84437926466414\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.27446382220158\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.00481773536545\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.59305502669805\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.2348204707096\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.44874950268269\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.53438747069652\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.84154725015549\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 121.90351873532626\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 121.7714482691165\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 122.34221426459887\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.23845255956567\n",
      "\n",
      "****************************************************************************************************\n",
      "Domain: female\n",
      "\n",
      "Number of instances: 100\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.21961530806594\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.48922593964853\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 156.05474659457158\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 147.98190649503962\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 83.44152891710812\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 53.396360075421256\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65038633086905\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.87574446239665\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 117.16704480518578\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 96.87683753165479\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.34493310664914\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 147.25205232585492\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 90.23933134720272\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 62.71570506936003\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.17739970914755\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.55459071079548\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.65038633086905\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 153.87574446239665\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.06976560322101\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.28521808115167\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.9604908209104\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 143.80605671868645\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.32256198393713\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 140.43826037500114\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.1559790923013\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 138.18182905009564\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.46074214600284\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.03676274397006\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 140.2368511450418\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.00306145662427\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.48430608941814\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 138.08072518805832\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.20310697913192\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 140.26975393827226\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.39325381418305\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 143.57014770726602\n",
      "\n",
      "Number of instances: 300\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.0993164003083\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.26651796748212\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.9066256096329\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 148.56003972100098\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 100.55006474755794\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 67.27162158181936\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.91824350849168\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.63484213104502\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.21881184008173\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.90193495249376\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 153.87809035905235\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.2583360514586\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 108.8610740513939\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 81.4941403614599\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.90312793447063\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.52626715915733\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.91824350849168\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 128.63484213104502\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.9397770232106\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.64738855990775\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.3562671369062\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.54437350657783\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.16771384957855\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.32579697105517\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.37411716122762\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.99165895333984\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.97547707185336\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.54195945343177\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.97179358145587\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.97669847133102\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.36306669003505\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.29587600703758\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 147.14929639759094\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 137.4994920605514\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.33048270412357\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 142.58754663187256\n",
      "\n",
      "Number of instances: 500\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.14482543554536\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.2351349841211\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.61876803702634\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 146.9163843107162\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 99.5982519787399\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 69.65852953873645\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.72835550578358\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.76692824677669\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.22640768318767\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 98.94428135710187\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 152.38593923087032\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 140.0257069104477\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 108.03190175504531\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 80.35338695266113\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.3792493665464\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.84870869235482\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.72835550578358\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.76692824677669\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.5731466704402\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.62319730460854\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 130.82777118808963\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.40361670656526\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 132.49222905873185\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.10818645264695\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 134.56652028236692\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.73690654285353\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 137.05064485899476\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 125.289776977185\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.94460278861544\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.76679775564143\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 143.24839407122894\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 131.16796887822275\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.96201870683527\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 135.49329034492897\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.08547669543438\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 140.74276215576015\n",
      "\n",
      "Number of instances: 700\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.02214805500313\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 96.57698235121475\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 155.65565297126798\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 150.61417911693667\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 100.97059713313894\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 72.55158626952057\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.34529733841792\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.5373495736272\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 116.1167364915292\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 97.33334823059417\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 151.18729026938456\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 139.88608052727216\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 109.36817042031873\n",
      "Best: 0.234644 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 110.54037015587019\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 93.48562968262281\n",
      "Best: -177.607549 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.001, 'solver': 'adam'}\n",
      "MSE Train: 83.89331498849577\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 135.0247952650334\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 132.82812831841167\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 127.34529733841792\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.5373495736272\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 128.34584901790765\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.25454601951968\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 129.75317489379634\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 122.83629502284293\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 131.567274966084\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 123.28259658359698\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 133.78814923477054\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 124.59345070178178\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 136.41579769985609\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 126.76885737739732\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 139.45022036134057\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 129.80881661044367\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 142.891417219224\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 133.71332840092074\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 146.73938827350636\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 138.48239274882863\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 150.99413352418773\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 144.11600965416724\n",
      "\n",
      "****************************************************************************************************\n",
      "Domain: mixed\n",
      "\n",
      "Number of instances: 100\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.95592932349885\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 96.7441413292188\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.89065353650298\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.2594125040941\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 74.28537430373756\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 47.59245261528479\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08974406206201\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.18358393584866\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.44978976730931\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 96.9041020987362\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.57268680662273\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.32082711480484\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 96.40429667122793\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 71.56563203051378\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.86622952012137\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 118.64003968950266\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 112.08974406206201\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 127.18358393584866\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 109.20817011773599\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 120.95557549140983\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 106.82918837158115\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 115.73547622502952\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.95279882359742\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 111.52328613670775\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.57900147378481\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.3190052264445\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.70779632214334\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.12263349423978\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.339183368673\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.9341709400936\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.47316261337379\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.75361756400594\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.10973405624573\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.5809733659768\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.2488976972888\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 107.4162383460062\n",
      "\n",
      "Number of instances: 300\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.67647645773596\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 96.61323301941569\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.83213666709197\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.54736935390954\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 104.51732564605246\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 80.1375544448323\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.64104064686177\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.79151445042278\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 120.95509495513242\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.71788914397037\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.21070531716825\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 110.06162354047883\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 115.26359060019391\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.37755064661552\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.83966866207969\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.62188126436395\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.64104064686177\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.79151445042278\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.73283415206917\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.59357032524368\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.03069790101341\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.9230772257374\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.53463189369444\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.78003515190396\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.24463613011228\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.16444410374339\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.16071061026689\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.07630408125567\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.28285533415833\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.51561508444075\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.61107030178654\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.48237711329868\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.14535551315156\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.97659016782946\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.88571096825335\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 107.9982542480331\n",
      "\n",
      "Number of instances: 500\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.8686002436775\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 96.45062273025788\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.93488532012597\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.92403821703738\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 107.47509829411659\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 81.31037291700333\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.47632573422732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.16650942182073\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.02624337699237\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.20406159790421\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.10243100297866\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.51088256545016\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 118.53978934336939\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 86.52927671550388\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.99660180289594\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.99413478495548\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.47632573422732\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 108.16650942182073\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.9796822411654\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 105.86277188056586\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.64803862624834\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.08780998837246\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.48139488947608\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.8416237452405\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.47975103084869\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.12421315117\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.64310705036617\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.93557820616094\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.97146294802843\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.27571891021334\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.46481872383555\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.14463526332717\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.1231743777875\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.54232726550245\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.94652990988432\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.4687949167392\n",
      "\n",
      "Number of instances: 700\n",
      "* CORAL SRCONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.58874341381002\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 97.12658999124628\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.93692591142447\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 109.06262312503735\n",
      "\n",
      "* CORAL TGTONLY Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 113.7501281337978\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 87.98561803162703\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.10762063861826\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.73806674939583\n",
      "\n",
      "* CORAL ALL Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 121.04045006310534\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 98.85425061689735\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.10023598344434\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.39379460880434\n",
      "\n",
      "* CORAL WEIGHT Results *\n",
      "--> Linear Model Lasso\n",
      "MSE Dev: 86.24903100732917\n",
      "Best: 0.157143 using {'alpha': 0.31622776601683794}\n",
      "MSE Train: 120.49298917621245\n",
      "--> Multi-layer Perceptron regressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Dev: 106.89924675960377\n",
      "Best: -126.539141 using {'alpha': 0.1, 'early_stopping': True, 'hidden_layer_sizes': 200, 'learning_rate': 'invscaling', 'learning_rate_init': 0.1, 'solver': 'adam'}\n",
      "MSE Train: 92.13032110291564\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.37070381600977\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.92151072385693\n",
      "\n",
      "* CORAL LININT Results *\n",
      "Weight:  0.0 1.0\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.10762063861826\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.73806674939583\n",
      "\n",
      "Weight:  0.1 0.9\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.91522669121116\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 103.07851196832247\n",
      "\n",
      "Weight:  0.2 0.8\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.82846040484576\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.88384839139077\n",
      "\n",
      "Weight:  0.30000000000000004 0.7\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.8473217795221\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.15407601860076\n",
      "\n",
      "Weight:  0.4 0.6\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 102.97181081524018\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 100.88919484995243\n",
      "\n",
      "Weight:  0.5 0.5\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.20192751199993\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.08920488544574\n",
      "\n",
      "Weight:  0.6000000000000001 0.3999999999999999\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.53767186980139\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 101.75410612508072\n",
      "\n",
      "Weight:  0.7000000000000001 0.29999999999999993\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 103.97904388864461\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 102.88389856885739\n",
      "\n",
      "Weight:  0.8 0.19999999999999996\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 104.5260435685295\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 104.47858221677569\n",
      "\n",
      "Weight:  0.9 0.09999999999999998\n",
      "\n",
      "--> Linear Model Lasso\n",
      "MSE Test: 105.17867090945612\n",
      "--> Multi-layer Perceptron regressor\n",
      "MSE Test: 106.53815706883567\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def algorithm_CORAL(source, target):\n",
    "    cov_source_train = np.cov(source.T) + np.identity(source.shape[1])\n",
    "    cov_target_train = np.cov(target.T) + np.identity(target.shape[1])\n",
    "    source = np.dot(source, sc_linalg.fractional_matrix_power(cov_source_train, -0.5))\n",
    "    X_source = np.dot(source, sc_linalg.fractional_matrix_power(cov_target_train, 0.5))\n",
    "    return X_source\n",
    "    \n",
    "def CORAL(target, instance_size):\n",
    "    dev_instance_size = 100\n",
    "    domains = ['male', 'female', 'mixed']\n",
    "    domains.remove(target)\n",
    "    source = domains\n",
    "    X_source_1_train, y_source_1_train, X_source_1_test, y_source_1_test, X_source_1_dev, y_source_1_dev = create_domains(source[0])\n",
    "    X_source_2_train, y_source_2_train, X_source_2_test, y_source_2_test, X_source_2_dev, y_source_2_dev = create_domains(source[1])\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "\n",
    "    print('* CORAL SRCONLY Results *')\n",
    "    source_train = pd.concat([X_source_1_train, X_source_2_train])\n",
    "    target_train = X_target_train[:instance_size]\n",
    "    X_source = algorithm_CORAL(source_train, target_train)\n",
    "    y_source = pd.concat([y_source_1_train, y_source_2_train])\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    src_predictions_1, src_predictions_2 = get_predictions_from_baseline(X_source, y_source, X_test, y_test, X_dev, y_dev)\n",
    "    \n",
    "    print('* CORAL TGTONLY Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    source_train = X_target_train[:instance_size]\n",
    "    target_train = X_target_train[:instance_size]\n",
    "    X_source = algorithm_CORAL(source_train, target_train)\n",
    "    y_source = y_target_train[:instance_size]\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    tgt_predictions_1, tgt_predictions_2 = get_predictions_from_baseline(X_source, y_source, X_test, y_test, X_dev, y_dev)\n",
    "    \n",
    "    print('* CORAL ALL Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    target_train = X_target_train[:instance_size]\n",
    "    y_target_train = y_target_train[:instance_size]\n",
    "    source_train = pd.concat([X_source_1_train, X_source_2_train, target_train])\n",
    "    X_source = algorithm_CORAL(source_train, target_train)\n",
    "    y_source = pd.concat([y_source_1_train, y_source_2_train, y_target_train])\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    get_predictions_from_baseline(X_source, y_source, X_test, y_test, X_dev, y_dev)\n",
    "    \n",
    "    print('* CORAL WEIGHT Results *')\n",
    "    X_target_train, y_target_train, X_target_test, y_target_test, X_target_dev, y_target_dev = create_domains(target)\n",
    "    target_train = X_target_train[:instance_size]\n",
    "    y_target_train = y_target_train[:instance_size]\n",
    "    source_train = pd.concat([X_source_1_train, X_source_2_train])\n",
    "    y_source = pd.concat([y_source_1_train, y_source_2_train])\n",
    "    downsample_source_train, y_train = downsampling_for_weight(source_train, y_source, target_train, y_target_train)\n",
    "    X_source = algorithm_CORAL(downsample_source_train, target_train)\n",
    "    y_target_train = y_target_train[:instance_size]\n",
    "    y_source = y_train\n",
    "    X_test = X_target_test\n",
    "    y_test = y_target_test\n",
    "    X_dev = X_target_dev[:dev_instance_size]\n",
    "    y_dev = y_target_dev[:dev_instance_size]\n",
    "    get_predictions_from_baseline(X_source, y_source, X_test, y_test, X_dev, y_dev)\n",
    "    \n",
    "    print('* CORAL LININT Results *')\n",
    "    find_best_weight(src_predictions_1, tgt_predictions_1, src_predictions_2, tgt_predictions_2, y_test)\n",
    "    \n",
    "def run_CORAL_with_n_instances():\n",
    "    print('*'*100)\n",
    "    print('CORAL')\n",
    "    print('*'*100)\n",
    "    domains = ['male', 'female', 'mixed']\n",
    "    n_instances = [100, 300, 500, 700]\n",
    "    \n",
    "    for domain in domains:\n",
    "        print('Domain:', domain)\n",
    "        print()\n",
    "        for instances in n_instances:\n",
    "            print('Number of instances:', instances)\n",
    "            CORAL(domain, instances) \n",
    "        print('*'*100)\n",
    "\n",
    "run_CORAL_with_n_instances()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
